{"cells":[{"cell_type":"markdown","metadata":{},"source":["**This code is base on [2.5d segmentaion baseline [inference]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-inference)**\n","If you think my code is useful,please upvote it ^w^."]},{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-12-12T13:56:06.145497Z","iopub.status.busy":"2023-12-12T13:56:06.145152Z","iopub.status.idle":"2023-12-12T13:56:27.976042Z","shell.execute_reply":"2023-12-12T13:56:27.975092Z","shell.execute_reply.started":"2023-12-12T13:56:06.145468Z"},"trusted":true},"outputs":[],"source":["# !python -m pip install /kaggle/input/install-cc3d/connected_components_3d-3.12.3-cp310-cp310-win_amd64.whl"]},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in links: /kaggle/input/pip-download-for-segmentation-models-pytorch\n","Requirement already satisfied: segmentation-models-pytorch in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (0.3.3)\n","Requirement already satisfied: torchvision>=0.5.0 in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (0.15.2)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (0.7.4)\n","Requirement already satisfied: efficientnet-pytorch==0.7.1 in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (0.7.1)\n","Requirement already satisfied: timm==0.9.2 in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (0.9.2)\n","Requirement already satisfied: tqdm in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (4.65.0)\n","Requirement already satisfied: pillow in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from segmentation-models-pytorch) (9.5.0)\n","Requirement already satisfied: torch in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.1+cu121)\n","Requirement already satisfied: munch in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n","Requirement already satisfied: pyyaml in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0)\n","Requirement already satisfied: huggingface-hub in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.15.1)\n","Requirement already satisfied: safetensors in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.3.1)\n","Requirement already satisfied: numpy in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.4)\n","Requirement already satisfied: requests in d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","\n","The conflict is caused by:\n","    efficientnet-pytorch 0.7.1 depends on torch\n","    pretrainedmodels 0.7.4 depends on torch\n","    timm 0.9.2 depends on torch>=1.7\n","    torchvision 0.15.2 depends on torch==2.0.1\n","\n","To fix this you could try to:\n","1. loosen the range of package versions you've specified\n","2. remove package versions to allow pip attempt to solve the dependency conflict\n","\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -illow (d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages)\n","WARNING: Ignoring invalid distribution -orch (d:\\applications\\miniconda3\\envs\\d2l\\lib\\site-packages)\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","WARNING: Location '/kaggle/input/pip-download-for-segmentation-models-pytorch' is ignored: it is either a non-existing path or lacks a specific scheme.\n","ERROR: Cannot install efficientnet-pytorch==0.7.1, pretrainedmodels==0.7.4, timm==0.9.2 and torchvision==0.15.2 because these package versions have conflicting dependencies.\n","ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"]}],"source":["import torch \n","import torch.nn as nn  \n","import numpy as np\n","from tqdm import tqdm\n","from torch.cuda.amp import autocast\n","import cv2\n","import os,sys\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","!python -m pip install --no-index --find-links=/kaggle/input/pip-download-for-segmentation-models-pytorch segmentation-models-pytorch\n","import segmentation_models_pytorch as smp\n","# import cc3d\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["# config"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","# ============== model CFG =============\n","    model_name = 'Unet'\n","    #backbone = 'efficientnet-b0'\n","    backbone = 'se_resnet50'\n","\n","    in_chans = 5 # 65\n","    # ============== training CFG =============\n","    image_size = 256\n","    input_size=256\n","    tile_size = image_size\n","    stride = tile_size // 2\n","    drop_egde_pixel=0\n","    \n","    target_size = 1\n","    # ============== fold =============\n","    valid_id = 1\n","    batch=64\n","    th_percentile = 0.002#0.005E:\\CSworks\\kaggle_blood_vessel\\models\\2023-12-10-baseline-v0\n","    model_path=[\"./models/2023-12-10-baseline-v0/se_resnet50_9_loss0.01_score0.76_val_loss0.01_val_scorenan.pt\"]"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, CFG, weight=None):\n","        super().__init__()\n","        self.CFG = CFG\n","        self.encoder = smp.Unet(\n","            encoder_name=CFG.backbone, \n","            encoder_weights=weight,\n","            in_channels=CFG.in_chans,\n","            classes=CFG.target_size,\n","            activation=None,\n","        )\n","\n","    def forward(self, image):\n","        output = self.encoder(image)\n","        # output = output.squeeze(-1)\n","        return output[:,0]#.sigmoid()\n","\n","\n","def build_model(weight=None):\n","    from dotenv import load_dotenv\n","    load_dotenv()\n","\n","    print('model_name', CFG.model_name)\n","    print('backbone', CFG.backbone)\n","\n","    model = CustomModel(CFG, weight)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["def rle_encode(mask):\n","    pixel = mask.flatten()\n","    pixel = np.concatenate([[0], pixel, [0]])\n","    run = np.where(pixel[1:] != pixel[:-1])[0] + 1\n","    run[1::2] -= run[::2]\n","    rle = ' '.join(str(r) for r in run)\n","    if rle == '':\n","        rle = '1 0'\n","    return rle\n","\n","def load_img(paths):\n","    output = []\n","    for path in paths:\n","        if path is None:\n","            output.append(None)\n","            continue\n","        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","        img = img.astype('float32') # original is uint16\n","        output.append(img)\n","    shape=[x.shape for x in output if x is not None][0]\n","    for i in range(len(output)):\n","        if output[i] is None:\n","            output[i] = torch.randn(shape)\n","    output=np.stack(output, axis=0)\n","    return torch.from_numpy(output)\n","\n","def min_max_normalization(x:torch.Tensor)->torch.Tensor:\n","    \"\"\"input.shape=(batch,f1,...)\"\"\"\n","    shape=x.shape\n","    if x.ndim>2:\n","        x=x.reshape(x.shape[0],-1)\n","    \n","    min_=x.min(dim=-1,keepdim=True)[0]\n","    max_=x.max(dim=-1,keepdim=True)[0]\n","    if min_.mean()==0 and max_.mean()==1:\n","        return x.reshape(shape)\n","    \n","    x=(x-min_)/(max_-min_+1e-9)\n","    return x.reshape(shape)\n","\n","class Data_loader(Dataset):\n","    def __init__(self,path,s=\"/images/\"):\n","        self.paths=glob(path+f\"{s}*.tif\")\n","        self.paths.sort()\n","        self.bool=s==\"/labels/\"\n","    \n","    def __len__(self):\n","        return len(self.paths)\n","    \n","    def __getitem__(self,index):\n","        img=cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n","        img=torch.from_numpy(img)\n","        if self.bool:\n","            img=img.to(torch.bool)\n","        else:\n","            img=img.to(torch.uint8)\n","        return img\n","\n","def load_data(path,s):\n","    data_loader=Data_loader(path,s)\n","    data_loader=DataLoader(data_loader, batch_size=16, num_workers=2)\n","    data=[]\n","    for x in tqdm(data_loader):\n","        data.append(x)\n","    return torch.cat(data,dim=0)    \n","\n","class Pipeline_Dataset(Dataset):\n","    def __init__(self,x,path,labels=None):\n","        self.img_paths  = glob(path+\"/images/*\")\n","        self.img_paths.sort()\n","        #assert int(self.img_paths[-1].split(\"/\")[-1][:-4])+1==len(x)\n","        self.debug=labels\n","        self.in_chan = 5\n","        z=torch.zeros(self.in_chan//2,*x.shape[1:],dtype=x.dtype)\n","        self.x=torch.cat((z,x,z),dim=0)\n","        self.labels=labels\n","        \n","    def __len__(self):\n","        return self.x.shape[0]-4\n","    \n","    def __getitem__(self, index):\n","        x  = self.x[index:index+self.in_chan]\n","        if self.labels is not None :\n","            label=self.labels[index]\n","        else:\n","            label=torch.zeros_like(x[0])\n","        #Normalization\n","        id=self.img_paths[index].split(\"/\")[-3:]\n","        id.pop(1)\n","        id=\"_\".join(id)\n","        #return img,torch.from_numpy(mask),id\n","        return x,label,id[:-4]\n","\n","def add_edge(x:torch.Tensor,edge:int):\n","    #x=(C,H,W)\n","    #output=(C,H+2*edge,W+2*edge)\n","    x=torch.cat([x,torch.ones([x.shape[0],edge,x.shape[2]],dtype=x.dtype,device=x.device)*128],dim=1)\n","    x=torch.cat([x,torch.ones([x.shape[0],x.shape[1],edge],dtype=x.dtype,device=x.device)*128],dim=2)\n","    x=torch.cat([torch.ones([x.shape[0],edge,x.shape[2]],dtype=x.dtype,device=x.device)*128,x],dim=1)\n","    x=torch.cat([torch.ones([x.shape[0],x.shape[1],edge],dtype=x.dtype,device=x.device)*128,x],dim=2)\n","    return x\n","\n","# def TTA(x:tc.Tensor,model:nn.Module,batch=CFG.batch):\n","#     x=x.to(tc.float32)\n","#     x=min_max_normalization(x)\n","#     #x.shape=(batch,c,h,w)\n","#     if CFG.input_size!=CFG.image_size:\n","#         x=nn.functional.interpolate(x,size=(CFG.input_size,CFG.input_size),mode='bilinear',align_corners=True)\n","    \n","#     shape=x.shape\n","#     x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","#     x=tc.cat(x,dim=0)\n","#     with autocast():\n","#         with tc.no_grad():\n","#             #x=[model((x[i*batch:(i+1)*batch],print(x[i*batch:(i+1)*batch].shape))[0]) for i in range(x.shape[0]//batch+1)]\n","#             x=[model(x[i*batch:(i+1)*batch]) for i in range(x.shape[0]//batch+1)]\n","#             # batch=64,64...48\n","#             x=tc.cat(x,dim=0)\n","#     x=x.sigmoid()\n","#     x=x.reshape(4,shape[0],*shape[2:])\n","#     x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","#     x=tc.stack(x,dim=0).mean(0)\n","    \n","#     if CFG.input_size!=CFG.image_size:\n","#         x=nn.functional.interpolate(x[None],size=(CFG.image_size,CFG.image_size),mode='bilinear',align_corners=True)[0]\n","#     return x\n","\n","\n","def TTA(x:torch.Tensor, model:nn.Module, batch=CFG.batch):\n","    x = x.to(torch.float32)\n","    x = min_max_normalization(x)  # 假设提前已定义\n","    # x.shape=(batch,c,h,w)\n","    \n","    # 如果input_size和image_size不同，则执行新的padding策略\n","    if CFG.input_size != CFG.image_size:\n","        pad_height = CFG.input_size - x.size(2)\n","        pad_width = CFG.input_size - x.size(3)\n","        # 在底部和右侧进行零填充\n","        padding = [pad_width, 0, pad_height, 0] # 右, 左, 底, 上\n","        x = nn.functional.pad(x, padding, 'constant', 0)\n","    \n","    shape = x.shape\n","    x = [x, *[torch.rot90(x, k=i, dims=(-2, -1)) for i in range(1, 4)]]\n","    x = torch.cat(x, dim=0)\n","    with autocast():\n","        with torch.no_grad():\n","            x=[model(x[i*batch:(i+1)*batch]) for i in range(x.shape[0]//batch+1)]\n","            x = torch.cat(x, dim=0)\n","    x = x.sigmoid()\n","    x = x.reshape(4, shape[0], *shape[2:])\n","    x = [torch.rot90(x[i], k=-i, dims=(-2, -1)) for i in range(4)]\n","    x = torch.stack(x, dim=0).mean(0)\n","    \n","    if CFG.input_size != CFG.image_size:\n","        # 如果需要，剪裁返回到原始图像大小\n","        x = x[:, pad_height:, :CFG.image_size]  # 裁剪掉底部和右侧填充的部分\n","    \n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["# Build model(s)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from torch.nn import DataParallel"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["def get_output(debug=False):\n","    outputs=[]\n","    if debug:\n","        paths=[\"./kaggle/input/blood-vessel-segmentation/train/kidney_2\"]\n","    else:\n","        paths=glob(\"./kaggle/input/blood-vessel-segmentation/test/*\")\n","    debug_count=0\n","    for path in paths:\n","        x=load_data(path,\"/images/\")\n","        dataset=Pipeline_Dataset(x,path,None)\n","        dataloader=DataLoader(dataset,batch_size=1,shuffle=debug,num_workers=2)\n","        for img,label,id in tqdm(dataloader):\n","            #print(label.shape)\n","            #img=(C,H,W)\n","            img=img.to(\"cuda:0\")\n","            label=label.to(\"cuda:0\")\n","            img=add_edge(img[0],CFG.tile_size//2)[None]\n","            label=add_edge(label,CFG.tile_size//2)\n","            x1_list = np.arange(0, label.shape[-2]-CFG.tile_size+1, CFG.stride)\n","            y1_list = np.arange(0, label.shape[-1]-CFG.tile_size+1, CFG.stride)\n","\n","            mask_pred = torch.zeros_like(label,dtype=torch.float32,device=label.device)\n","            mask_count = torch.zeros_like(label,dtype=torch.float32,device=label.device)\n","\n","            indexs=[]\n","            chip=[]\n","            for y1 in y1_list:\n","                for x1 in x1_list:\n","                    x2 = x1 + CFG.tile_size\n","                    y2 = y1 + CFG.tile_size\n","                    indexs.append([x1+CFG.drop_egde_pixel,x2-CFG.drop_egde_pixel,\n","                                   y1+CFG.drop_egde_pixel,y2-CFG.drop_egde_pixel])\n","                    chip.append(img[...,x1:x2,y1:y2])\n","\n","            y_preds = TTA(torch.cat(chip),model)\n","            if CFG.drop_egde_pixel:\n","                y_preds=y_preds[...,CFG.drop_egde_pixel:-CFG.drop_egde_pixel,\n","                                    CFG.drop_egde_pixel:-CFG.drop_egde_pixel]\n","            for i,(x1,x2,y1,y2) in enumerate(indexs):\n","                mask_pred[...,x1:x2, y1:y2] += y_preds[i]\n","                mask_count[...,x1:x2, y1:y2] += 1\n","\n","            mask_pred /= mask_count\n","\n","            #Rrecover\n","            mask_pred=mask_pred[...,CFG.tile_size//2:-CFG.tile_size//2,CFG.tile_size//2:-CFG.tile_size//2]\n","            label=label[...,CFG.tile_size//2:-CFG.tile_size//2,CFG.tile_size//2:-CFG.tile_size//2]\n","\n","            outputs.append(((mask_pred*255).to(torch.uint8).cpu().numpy()[0],id))\n","            if debug:\n","                debug_count+=1\n","                plt.subplot(121)\n","                plt.imshow(img[0,2].cpu().detach().numpy())\n","                plt.subplot(122)\n","                plt.imshow(mask_pred[0].cpu().detach().numpy())\n","                plt.show()\n","                if debug_count>6:\n","                    break\n","    return outputs\n","    "]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["# is_submit=len(glob(\"/kaggle/input/blood-vessel-segmentation/test/kidney_5/images/*.tif\"))!=3\n","# outputs=get_output(not is_submit)\n","\n","# TH = [output.flatten() for output,id in outputs] \n","# TH = np.concatenate(TH)\n","# index = -int(len(TH) * CFG.th_percentile)\n","# TH:int = np.partition(TH, index)[index]\n","# print(TH)\n","# submission_df=[]\n","# for mask_pred,id in outputs:\n","#     if not is_submit:\n","#         plt.subplot(121)\n","#         plt.imshow(mask_pred)\n","#         plt.subplot(122)\n","#         plt.imshow(mask_pred>TH)\n","#         plt.show()\n","#     mask_pred=mask_pred>TH\n","#     rle = rle_encode(mask_pred)\n","    \n","#     submission_df.append(\n","#         pd.DataFrame(data={\n","#             'id'  : id,\n","#             'rle' : rle,\n","#         })\n","#     )\n","\n","# submission_df =pd.concat(submission_df)\n","# submission_df.to_csv('submission.csv', index=False)\n","# submission_df.head(6)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IMPORT OK  !!!!\n"]}],"source":["import sys, os\n","# sys.path.append('/kaggle/input/blood-vessel-segmentation-third-party')\n","# sys.path.append('/kaggle/input/blood-vessel-segmentation-00')\n","\n","# from helper import *\n","\n","import cv2\n","import pandas as pd\n","from glob import glob\n","import numpy as np\n","\n","from timeit import default_timer as timer\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","print('IMPORT OK  !!!!')\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["class dotdict(dict):\n","    \"\"\"Simple dot-accessible dictionary.\"\"\"\n","    \n","    def __getattr__(self, attr):\n","        return self.get(attr)\n","    \n","    def __setattr__(self, key, value):\n","        self[key] = value\n","        \n","    def __delattr__(self, key):\n","        del self[key]"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(valid_meta) : 1\n","['./kaggle/input/blood-vessel-segmentation/train/kidney_3_sparse/images/0496.tif', './kaggle/input/blood-vessel-segmentation/train/kidney_3_sparse/images/0497.tif', './kaggle/input/blood-vessel-segmentation/train/kidney_3_sparse/images/0498.tif']\n","MODE OK  !!!!\n"]}],"source":["cfg = dotdict(\n","    batch_size = 3,\n","    p_threshold = 0.002,\n","    cc_threshold = -1,\n",")\n","\n","mode = 'local' # 'local' #\n","\n","data_dir = \\\n","    './kaggle/input/blood-vessel-segmentation'\n","\n","#-----\n","def file_to_id(f):\n","    s = f.split('/')\n","    return s[-3]+'_' + s[-1][:-4]\n","\n","if 'local' in mode:\n","    valid_folder = [\n","        ('kidney_3_sparse', (496, 996+1)),\n","        #('kidney_1_dense', (0, 1000+1)),\n","    ] #debug for local development\n","    \n","    valid_meta = []\n","    for image_folder, image_no in valid_folder:\n","        file = [f'{data_dir}/train/{image_folder}/images/{i:04d}.tif' for i in range(*image_no)]\n","        H,W = cv2.imread(file[0],cv2.IMREAD_GRAYSCALE).shape\n","        valid_meta.append(dotdict(\n","            name  = image_folder,\n","            file  = file,\n","            shape = (len(file), H, W),\n","            id = [file_to_id(f) for f in file],\n","        ))\n","        \n","if 'submit' in mode:\n","    valid_meta = []\n","    valid_folder = sorted(glob(f'{data_dir}/test/*'))\n","    for image_folder in valid_folder:\n","        file = sorted(glob(f'{image_folder}/images/*.tif'))\n","        H, W = cv2.imread(file[0], cv2.IMREAD_GRAYSCALE).shape\n","        valid_meta.append(dotdict(\n","            name=image_folder,\n","            file=file,\n","            shape=(len(file), H, W),\n","            id=[file_to_id(f) for f in file],\n","        ))\n","\n","#     glob_file = glob(f'{data_dir}/kidney_5/images/*.tif')\n","#     if len(glob_file)==3:\n","#         mode = 'submit-fake' #fake submission to save gpu time when submitting\n","#         #todo .....\n","\n","\n","\n","\n","print('len(valid_meta) :', len(valid_meta))\n","print(valid_meta[0].file[:3])\n","\n","print('MODE OK  !!!!')"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["if 'submit' in mode:\n","    valid_meta = []\n","    valid_folder = sorted(glob(f'{data_dir}/test/*'))\n","    for image_folder in valid_folder:\n","        file = sorted(glob(f'{image_folder}/images/*.tif'))\n","        H, W = cv2.imread(file[0], cv2.IMREAD_GRAYSCALE).shape\n","        valid_meta.append(dotdict(\n","            name=image_folder,\n","            file=file,\n","            shape=(len(file), H, W),\n","            id=[file_to_id(f) for f in file],\n","        ))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DATASET OK  !!!!\n"]}],"source":["class MyLoader(object):\n","    def __init__(self, meta):\n","        self.meta  = meta\n","        self.split = np.array_split(meta.file, max(1,int(len(meta.file)//cfg.batch_size)))\n","\n","    def __len__(self,):\n","        return len(self.split)\n","\n","    def __getitem__(self, index):\n","        file = self.split[index]\n","\n","        image = []\n","        for f in file:\n","            m = cv2.imread(f,cv2.IMREAD_GRAYSCALE)\n","\n","            #---\n","            #process image\n","            m = (m - m.min())/(m.max() - m.min() +0.001)\n","\n","            #---\n","            image.append(m)\n","\n","        image = np.stack(image)\n","        image = torch.from_numpy(image).float().unsqueeze(1)\n","        return image\n","\n","print('DATASET OK  !!!!')"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def make_dummy_submission(): \n","    submission_df = []\n","    for d in valid_meta: \n","        submission_df.append(\n","            pd.DataFrame(data={\n","                'id'  : d['id'],\n","                'rle' : ['1 0']*len(d['id']),\n","            })\n","        )\n","    submission_df =pd.concat(submission_df)\n","    submission_df.to_csv('submission.csv', index=False)\n","    print(submission_df)\n","    \n","\n","#https://www.kaggle.com/competitions/blood-vessel-segmentation/discussion/456033\n","def choose_biggest_object(mask, threshold):\n","    mask = ((mask > threshold) * 255).astype(np.uint8)\n","    num_label, label, stats, centroid = cv2.connectedComponentsWithStats(mask, connectivity=8)\n","    max_label = -1\n","    max_area = -1\n","    for l in range(1, num_label):\n","        if stats[l, cv2.CC_STAT_AREA] >= max_area:\n","            max_area = stats[l, cv2.CC_STAT_AREA]\n","            max_label = l\n","    processed = (label==max_label).astype(np.uint8)\n","    return processed\n","\n","\n","def remove_small_objects(mask, min_size, threshold):\n","    mask = ((mask > threshold) * 255).astype(np.uint8)\n","    # find all connected components (labels)\n","    num_label, label, stats, centroid = cv2.connectedComponentsWithStats(mask, connectivity=8)\n","    # create a mask where small objects are removed\n","    processed = np.zeros_like(mask)\n","    for l in range(1, num_label):\n","        if stats[l, cv2.CC_STAT_AREA] >= min_size:\n","            processed[label == l] = 1\n","    return processed\n","\n","\n","def rle_encode(mask):\n","    pixel = mask.flatten()\n","    pixel = np.concatenate([[0], pixel, [0]])\n","    run = np.where(pixel[1:] != pixel[:-1])[0] + 1\n","    run[1::2] -= run[::2]\n","    rle = ' '.join(str(r) for r in run)\n","    if rle == '':\n","        rle = '1 0'\n","    return rle\n","\n","#-------------------------------"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model_name Unet\n","backbone se_resnet50\n"]},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn [34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m net \u001b[38;5;241m=\u001b[39m DataParallel(net)\n\u001b[0;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(CFG\u001b[38;5;241m.\u001b[39mmodel_path[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m net\u001b[38;5;241m=\u001b[39m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[1;31m[... skipping similar frames: _apply at line 641 (2 times)]\u001b[0m\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32md:\\Applications\\Miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["net=build_model()\n","net = DataParallel(net)\n","net.load_state_dict(torch.load(CFG.model_path[0],\"cpu\"))\n","net=net.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","# from model_2cls_50 import *\n","# checkpoint_file = \\\n","#     '/public/sist/home/hongmt2022/MyWorks/kaggle-bv/models/baseline-v0-2023-12-20/se_resnet50_9_loss0.01_score0.76_val_loss0.01_val_scorenan.pt'\n","    #'/kaggle/input/blood-vessel-segmentation-00/00001085.pth'\n","    #'/kaggle/input/blood-vessel-segmentation-00/00000966.pth'\n","\n","# net = Net()\n","# #run_check_net()\n","# state_dict = torch.load(checkpoint_file, map_location=lambda storage, loc: storage)['state_dict']\n","# print(net.load_state_dict(state_dict, strict=False))  # True\n","\n","\n","#net = torch.compile(net)\n","\n","\"\"\"\n","1. 函数`do_submit()`定义了一个提交函数。\n","\n","2. 创建一个空列表`submission_df`，用于最后将所有的预测结果合并成一个DataFrame对象。\n","\n","3. 通过一个for循环遍历在`valid_meta`中的每个元素（可能代表一个患者的医学影像数据）。\n","\n","4. 读取该元素所关联的体积影像，体积影像由多个切片组成。\n","\n","5. 将这些切片堆叠成一个三维numpy数组`volume`。\n","\n","6. 获取体积影像的深度（D）、高度（H）、宽度（W）。\n","\n","7. 创建一个和影像体积相同形状全零的numpy数组`predict`，用于保存预测结果。\n","\n","8. 定义需要遍历的轴`axes`。\n","\n","9. 通过三个不同的方向（轴0、轴1、轴2）遍历影像体积。\n","\n","10. 计算每个轴的分块索引，并赋值给变量`loader`。\n","\n","11. 初始化计时器`start_timer`。\n","\n","12. 使用两层嵌套的for循环批量处理图片数据。\n","\n","13. 打印进度信息。\n","\n","14. 根据当前的轴，获取batch的子集`image`，并可能执行必要的转置操作（针对轴1或轴2）。\n","\n","15. 对影像数据标准化，使其值分布在0到1之间。\n","\n","16. 将numpy数组转换为PyTorch张量，并移动到GPU上。\n","\n","17. 进入混合精度执行环境，通常用于辅助GPU以提高效率。\n","\n","18. 在没有梯度的情况下执行预测。\n","\n","19. 网络`net`对图片数据`image`做出预测，获取血管和肾脏的预测结果`vessel`和`kidney`。\n","\n","20. 通过四次数据增强（水平、垂直翻转和两次旋转）来增加模型预测的可靠性，并将结果相加。\n","\n","21. 将预测值归一化（即计数器`counter`）。\n","\n","22. 将预测结果从GPU转移到CPU，并将其转换回numpy格式。\n","\n","23. 用预测结果更新`predict`数组。\n","\n","24. 如果配置允许，使用`choose_biggest_object`函数选择超过某个阈值的最大物体。\n","\n","25. 对局部模式（可能是调试模式）下的首个批量结果进行可视化显示。\n","\n","26. 更新处理过的批量大小`B`。\n","\n","27. 综合所有轴方向的预测结果，并应用一个阈值来二值化预测结果。\n","\n","28. 如果配置指定了连通性分量阈值，则移除体积小于此阈值的连通组件。\n","\n","29. 使用RLE编码压缩每个预测，并将结果保存到DataFrame中。\n","\n","30. 将每个患者的结果添加到`submission_df`列表中。\n","\n","31. 将结果列表合并为一个DataFrame，并将其保存为csv文件。\n","\n","32. 打印最终的提交DataFrame。\n","\"\"\"\n","def do_submit():\n","    \n","    submission_df = []\n","    for d in valid_meta:\n","        volume = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in d.file]\n","        volume = np.stack(volume)\n","        D, H, W = volume.shape\n","        \n","        predict = np.zeros(d.shape, dtype=np.float16)\n","        axes = [0,1,2] #[2]  # \n","        for axis in axes:  # 0\n","            loader = np.array_split(np.arange((D, H, W)[axis]), max(1, int((D, H, W)[axis] // cfg.batch_size)))\n","            num_valid = len(loader)\n","            \n","            B = 0 \n","            start_timer = timer()\n","            for t in range(num_valid):\n","                # print(f'\\r validation: {t}/{num_valid}', time_to_str(timer() - start_timer, 'min'), end='', flush=True)\n","\n","                if axis == 0:\n","                    image = volume[loader[t].tolist()]\n","                if axis == 1:\n","                    image = volume[:, loader[t].tolist()]\n","                    image = image.transpose(1, 0, 2)\n","                if axis == 2:\n","                    image = volume[:, :, loader[t].tolist()]\n","                    image = image.transpose(2, 0, 1)\n","\n","                batch_size, bh, bw = image.shape\n","                m = image.reshape(batch_size, -1)\n","                m = (m - m.min(keepdims=True)) / (m.max(keepdims=True) - m.min(keepdims=True) + 0.001)\n","                m = m.reshape(batch_size, bh, bw)\n","                m = np.ascontiguousarray(m)\n","                image = torch.from_numpy(m).float().cuda().unsqueeze(1)\n","\n","                #----\n","                counter = 0\n","                vessel, kidney = 0, 0\n","                image = image.cuda() \n","                with torch.cuda.amp.autocast(enabled=True):\n","                    with torch.no_grad():\n","                        v, k = net(image)\n","                        vessel += v\n","                        kidney += k\n","                        counter += 1\n","\n","                        v, k = net(torch.flip(image, dims=[2,]))\n","                        vessel += torch.flip(v, dims=[2,])\n","                        kidney += torch.flip(k, dims=[2,])\n","                        counter += 1\n","\n","                        v, k = net(torch.flip(image, dims=[3,]))\n","                        vessel += torch.flip(v, dims=[3,])\n","                        kidney += torch.flip(k, dims=[3,])\n","                        counter += 1\n","\n","                        v, k = net(torch.rot90(image, k=1, dims=[2,3]))\n","                        vessel += torch.rot90(v, k=-1, dims=[2,3])\n","                        kidney += torch.rot90(k, k=-1, dims=[2,3])\n","                        counter += 1\n","\n","                        v, k = net(torch.rot90(image, k=2, dims=[2,3]))\n","                        vessel += torch.rot90(v, k=-2, dims=[2,3])\n","                        kidney += torch.rot90(k, k=-2, dims=[2,3])\n","                        counter += 1\n","\n","                        v, k = net(torch.rot90(image, k=3, dims=[2,3]))\n","                        vessel += torch.rot90(v, k=-3, dims=[2,3])\n","                        kidney += torch.rot90(k, k=-3, dims=[2,3])\n","                        counter += 1\n","\n","                vessel = vessel/counter   \n","                kidney = kidney/counter      \n","                #print(i, image.shape, mask.shape) \n","\n","                vessel = vessel.float().data.cpu().numpy()\n","                kidney = kidney.float().data.cpu().numpy()\n","\n","                # ----------------------------------------\n","                batch_size = len(vessel)\n","                for b in range(batch_size):\n","                    mk = kidney[b, 0]\n","                    mk = choose_biggest_object(mk, threshold=0.5) \n","                    mv = vessel[b, 0]\n","                    p = (mv * mk)\n","                    if axis == 0:\n","                        predict[B + b] += p\n","                    if axis == 1:\n","                        predict[:, B + b] += p\n","                    if axis == 2:\n","                        predict[:, :, B + b] += p\n","\n","                    #debug only\n","                    if (t==0) and (mode=='local'): \n","                  \n","                        m = image[b, 0].float().data.cpu().numpy()\n","                        #p = predict[B+b]\n","\n","                        plt.imshow(np.hstack([m,p]),cmap='gray')\n","                        plt.show()\n","                        #plt.waitforbuttonpress()\n","\n","                #----------------------------------------\n","                B += batch_size\n","\n","        print('')\n","        predict = predict / len(axes)\n","        predict = (predict>cfg.p_threshold).astype(np.uint8)\n","\n","        #post processing ---\n","#         if cfg.cc_threshold>0:\n","#             predict = cc3d.dust(\n","#                 predict,\n","#                 connectivity=26,\n","#                 threshold=cfg.cc_threshold,\n","#                 in_place=False\n","#             )\n","\n","        rle = [rle_encode(p) for p in predict]\n","        submission_df.append(\n","            pd.DataFrame(data={\n","                'id'  : d['id'],\n","                'rle' : rle,\n","            })\n","        )\n","\n","    submission_df =pd.concat(submission_df)\n","    submission_df.to_csv('submission.csv', index=False)\n","    print(submission_df)\n","    \n","\n","glob_file = glob(f'{data_dir}/test/kidney_5/images/*.tif')\n","if (mode=='submit') and (len(glob_file)==3): #cannot do 3d cnn because too few test files\n","    make_dummy_submission()\n","else:\n","    do_submit()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6962461,"sourceId":61446,"sourceType":"competition"},{"datasetId":4087873,"sourceId":7147600,"sourceType":"datasetVersion"},{"datasetId":4139774,"sourceId":7166249,"sourceType":"datasetVersion"},{"sourceId":150248402,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":4}
