{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FPN\n","# 6 version of 2.5d Cutting model baseline [training]\n","#  author YOYOBAR\n","https://www.kaggle.com/code/yoyobar/2-5d-cutting-model-baseline-training"]},{"cell_type":"markdown","metadata":{},"source":["**This code is base on [2.5d segmentaion baseline [training]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-training)**\n","If you think my code is useful,please upvote it ^w^.\n","* Version6:\n","1. *     using kidney_1_dense for training and kidney_3_dense for val\n","2. *     image_size = 512\n","3. *     useing DiceLoss\n","4. *     norm_with_clip\n","5. *     fix some bug\n","\n","\n","* This version is correspond with [2.5d Cutting model baseline [inference]](https://www.kaggle.com/code/yoyobar/2-5d-cutting-model-baseline-inference) version3\n","\n","\n","\n","According to my experiments, using kidney_1_dense for training and kidney_3_dense for val is the best. You can even get 0.757, but using 2d model(se_resnext50_32x4d), you can set CFG.in_chans=1 to make this notebook as a 2d model training notebook.'"]},{"cell_type":"markdown","metadata":{},"source":["# Import"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-11T20:23:17.294857Z","iopub.status.busy":"2024-01-11T20:23:17.294087Z","iopub.status.idle":"2024-01-11T20:24:00.516912Z","shell.execute_reply":"2024-01-11T20:24:00.515893Z","shell.execute_reply.started":"2024-01-11T20:23:17.294799Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/public/sist/home/hongmt2022/k-bv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch as tc \n","import torch.nn as nn  \n","import numpy as np\n","from tqdm import tqdm\n","import os,sys,cv2\n","from torch.cuda.amp import autocast\n","import matplotlib.pyplot as plt\n","import albumentations as A\n","import segmentation_models_pytorch as smp\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.parallel import DataParallel\n","from glob import glob"]},{"cell_type":"markdown","metadata":{},"source":["# config"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:24:00.519332Z","iopub.status.busy":"2024-01-11T20:24:00.519017Z","iopub.status.idle":"2024-01-11T20:24:00.529838Z","shell.execute_reply":"2024-01-11T20:24:00.528767Z","shell.execute_reply.started":"2024-01-11T20:24:00.519303Z"},"trusted":true},"outputs":[],"source":["p_augm = 0.05 #0.5\n","#add rotate.  less p_augm\n","\n","class CFG:\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model CFG =============\n","    model_name = 'Unet'\n","    backbone = 'se_resnext50_32x4d'\n","\n","    in_chans = 1   #5 # 65\n","    # ============== training CFG =============\n","    image_size = 1024 # 512 # 512\n","    input_size = 1024 # 512 #=512\n","\n","    train_batch_size = 1 #4 #16\n","    valid_batch_size = train_batch_size * 2\n","\n","    epochs = 41 #27 #30 #25\n","    lr = 8e-5\n","    chopping_percentile=1e-3\n","    # ============== fold =============\n","    valid_id = 1\n","\n","\n","    # ============== augmentation =============\n","    train_aug_list = [\n","        A.Rotate(limit=270, p= 0.5),\n","        A.RandomScale(scale_limit=(0.8,1.25),interpolation=cv2.INTER_CUBIC,p=p_augm),\n","        A.RandomCrop(input_size, input_size,p=1),\n","        A.RandomGamma(p=p_augm*2/3),\n","        A.RandomBrightnessContrast(p=p_augm,),\n","        A.GaussianBlur(p=p_augm),\n","        A.MotionBlur(p=p_augm),\n","        A.GridDistortion(num_steps=5, distort_limit=0.3, p=p_augm),\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","    train_aug = A.Compose(train_aug_list)\n","    valid_aug_list = [\n","        ToTensorV2(transpose_mask=True),\n","    ]\n","    valid_aug = A.Compose(valid_aug_list)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:24:00.531473Z","iopub.status.busy":"2024-01-11T20:24:00.53115Z","iopub.status.idle":"2024-01-11T20:24:00.544468Z","shell.execute_reply":"2024-01-11T20:24:00.543548Z","shell.execute_reply.started":"2024-01-11T20:24:00.531448Z"},"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, CFG, weight=None):\n","        super().__init__()\n","        self.model = smp.FPN(  #FPN Unet\n","            encoder_name=CFG.backbone, \n","            encoder_weights=weight,\n","            in_channels=CFG.in_chans,\n","            classes=CFG.target_size,\n","            activation=None,\n","        )\n","\n","    def forward(self, image):\n","        output = self.model(image)\n","        # output = output.squeeze(-1)\n","        return output[:,0]#.sigmoid()\n","\n","\n","def build_model(weight=\"imagenet\"):\n","    from dotenv import load_dotenv\n","    load_dotenv()\n","\n","    print('model_name', CFG.model_name)\n","    print('backbone', CFG.backbone)\n","\n","    model = CustomModel(CFG, weight)\n","\n","    return model.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["# Change size"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:24:00.547193Z","iopub.status.busy":"2024-01-11T20:24:00.546908Z","iopub.status.idle":"2024-01-11T20:24:00.561517Z","shell.execute_reply":"2024-01-11T20:24:00.560608Z","shell.execute_reply.started":"2024-01-11T20:24:00.547169Z"},"trusted":true},"outputs":[],"source":["\n","def to_1024(img , image_size = 1024):\n","    if image_size > img.shape[1]:\n","       img = np.rot90(img)\n","       start1 = (CFG.image_size - img.shape[0])//2 \n","       top =     img[0                    : start1,   0: img.shape[1] ]\n","       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n","       img_result = np.concatenate((top,img,bottom ),axis=0)\n","       img_result = np.rot90(img_result)\n","       img_result = np.rot90(img_result)\n","       img_result = np.rot90(img_result)\n","    else :\n","       img_result = img\n","    return img_result\n","\n","def to_1024_no_rot(img, image_size = 1024):\n","    if image_size > img.shape[0]:  \n","       start1 = ( image_size - img.shape[0])//2\n","       top =     img[0                    : start1,   0: img.shape[1] ]\n","       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n","       img_result = np.concatenate((top,img,bottom ),axis=0)\n","    else: \n","       img_result = img\n","    return img_result\n","\n","#  add border\n","def to_1024_1024(img  , image_size = 1024 ):\n","     img_result = to_1024(img, image_size )\n","     return img_result\n","    \n","#  drop border\n","def to_original ( im_after, img, image_size = 1024 ):\n","    top_ = 0\n","    left_ = 0\n","    if (im_after.shape[0] > img.shape[0]):\n","             top_  = ( image_size - img.shape[0])//2 \n","    if    (im_after.shape[1] > img.shape[1]) :\n","             left_  = ( image_size - img.shape[1])//2  \n","    if (top_>0)or (left_>0) :\n","             img_result = im_after[top_                    : img.shape[0] + top_,   left_: img.shape[1] + left_ ]\n","             #print(im_after.shape,'-->',img_result.shape)\n","    else:\n","             img_result = im_after\n","    return img_result  "]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:24:00.563387Z","iopub.status.busy":"2024-01-11T20:24:00.563036Z","iopub.status.idle":"2024-01-11T20:24:00.601994Z","shell.execute_reply":"2024-01-11T20:24:00.600635Z","shell.execute_reply.started":"2024-01-11T20:24:00.563355Z"},"trusted":true},"outputs":[],"source":["def min_max_normalization(x:tc.Tensor)->tc.Tensor:\n","    \"\"\"input.shape=(batch,f1,...)\"\"\"\n","    shape=x.shape\n","    if x.ndim>2:\n","        x=x.reshape(x.shape[0],-1)\n","    \n","    min_=x.min(dim=-1,keepdim=True)[0]\n","    max_=x.max(dim=-1,keepdim=True)[0]\n","    if min_.mean()==0 and max_.mean()==1:\n","        return x.reshape(shape)\n","    \n","    x=(x-min_)/(max_-min_+1e-9)\n","    return x.reshape(shape)\n","\n","def norm_with_clip(x:tc.Tensor,smooth=1e-5):\n","    dim=list(range(1,x.ndim))\n","    mean=x.mean(dim=dim,keepdim=True)\n","    std=x.std(dim=dim,keepdim=True)\n","    x=(x-mean)/(std+smooth)\n","    x[x>5]=(x[x>5]-5)*1e-3 +5\n","    x[x<-3]=(x[x<-3]+3)*1e-3-3\n","    return x\n","\n","def add_noise(x:tc.Tensor,max_randn_rate=0.1,randn_rate=None,x_already_normed=False):\n","    \"\"\"input.shape=(batch,f1,f2,...) output's var will be normalizate  \"\"\"\n","    ndim=x.ndim-1\n","    if x_already_normed:\n","        x_std=tc.ones([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n","        x_mean=tc.zeros([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n","    else: \n","        dim=list(range(1,x.ndim))\n","        x_std=x.std(dim=dim,keepdim=True)\n","        x_mean=x.mean(dim=dim,keepdim=True)\n","    if randn_rate is None:\n","        randn_rate=max_randn_rate*np.random.rand()*tc.rand(x_mean.shape,device=x.device,dtype=x.dtype)\n","    cache=(x_std**2+(x_std*randn_rate)**2)**0.5\n","    #https://blog.csdn.net/chaosir1991/article/details/106960408\n","    \n","    return (x-x_mean+tc.randn(size=x.shape,device=x.device,dtype=x.dtype)*randn_rate*x_std)/(cache+1e-7)\n"," \n","class Data_loader(Dataset):\n","     \n","    def __init__(self,paths,is_label):\n","        self.paths=paths\n","        self.paths.sort()\n","        self.is_label=is_label\n","    \n","    def __len__(self):\n","        return len(self.paths)\n","    \n","    def __getitem__(self,index):\n","         \n","        img = cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n","        \n","        img = to_1024_1024(img , image_size = CFG.image_size ) #  to_original( im_after, img_save, image_size = 1024)\n","\n","        img = tc.from_numpy(img.copy())\n","        if self.is_label:\n","            img=(img!=0).to(tc.uint8)*255\n","        else:\n","            img=img.to(tc.uint8)\n","        return img\n","\n","def load_data(paths,is_label=False):\n","    data_loader=Data_loader(paths,is_label)\n","    data_loader=DataLoader(data_loader, batch_size=16, num_workers=2)  \n","    data=[]\n","    for x in tqdm(data_loader):\n","        data.append(x)\n","    x=tc.cat(data,dim=0)\n","    del data\n","    if not is_label:\n","        ########################################################################\n","        TH=x.reshape(-1).numpy()\n","        index = -int(len(TH) * CFG.chopping_percentile)\n","        TH:int = np.partition(TH, index)[index]\n","        x[x>TH]=int(TH)\n","        ########################################################################\n","        TH=x.reshape(-1).numpy()\n","        index = -int(len(TH) * CFG.chopping_percentile)\n","        TH:int = np.partition(TH, -index)[-index]\n","        x[x<TH]=int(TH)\n","        ########################################################################\n","        x=(min_max_normalization(x.to(tc.float16)[None])[0]*255).to(tc.uint8)\n","    return x\n","\n","\n","#https://www.kaggle.com/code/kashiwaba/sennet-hoa-train-unet-simple-baseline\n","def dice_coef(y_pred:tc.Tensor,y_true:tc.Tensor, thr=0.5, dim=(-1,-2), epsilon=0.001):\n","    y_pred=y_pred.sigmoid()\n","    y_true = y_true.to(tc.float32)\n","    y_pred = (y_pred>thr).to(tc.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n","    return dice\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = inputs.sigmoid()   \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        return 1 - dice\n","\n","class Kaggld_Dataset(Dataset):\n","    def __init__(self,x:list,y:list,arg=False):\n","        super(Dataset,self).__init__()\n","        self.x=x#list[(C,H,W),...]\n","        self.y=y#list[(C,H,W),...]\n","        self.image_size=CFG.image_size\n","        self.in_chans=CFG.in_chans\n","        self.arg=arg\n","        if arg:\n","            self.transform=CFG.train_aug\n","        else: \n","            self.transform=CFG.valid_aug\n","\n","    def __len__(self) -> int:\n","        return sum([y.shape[0]-self.in_chans for y in self.y])\n","    \n","    def __getitem__(self,index):\n","        i=0\n","        for x in self.x:\n","            if index>x.shape[0]-self.in_chans:\n","                index-=x.shape[0]-self.in_chans\n","                i+=1\n","            else:\n","                break\n","        x=self.x[i]\n","        y=self.y[i]\n","        \n","        print (f'x.shape[1] ={x.shape[1]}    x.shape[2]={x.shape[2]}')\n","        \n","        x_index= (x.shape[1]-self.image_size)//2 #np.random.randint(0,x.shape[1]-self.image_size)\n","        y_index= (x.shape[2]-self.image_size)//2 # np.random.randint(0,x.shape[2]-self.image_size)\n","        # i i+5 \n","        x=x[index:index+self.in_chans   ,   x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n","        # i+2\n","        y=y[index+self.in_chans//2   ,      x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n","\n","        data = self.transform(image=x.numpy().transpose(1,2,0), mask=y.numpy())\n","        x = data['image']\n","        y = data['mask']>=127\n","        if self.arg:\n","            i=np.random.randint(4)\n","            x=x.rot90(i,dims=(1,2))\n","            y=y.rot90(i,dims=(0,1))\n","            for i in range(3):\n","                if np.random.randint(2):\n","                    x=x.flip(dims=(i,))\n","                    if i>=1:\n","                        y=y.flip(dims=(i-1,))\n","        return x,y#(uint8,uint8)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load data "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:24:00.603949Z","iopub.status.busy":"2024-01-11T20:24:00.603522Z","iopub.status.idle":"2024-01-11T20:27:45.285906Z","shell.execute_reply":"2024-01-11T20:27:45.284648Z","shell.execute_reply.started":"2024-01-11T20:24:00.603915Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"torch.cat(): expected a non-empty list of Tensors","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/blood-vessel-segmentation/train/kidney_3_dense\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/images/*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m y\u001b[38;5;241m=\u001b[39mload_data(glob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/labels/*\u001b[39m\u001b[38;5;124m\"\u001b[39m),is_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(paths, is_label)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     69\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m---> 70\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_label:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m########################################################################\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"]}],"source":["train_x=[]\n","train_y=[]\n","\n","root_path=\"/kaggle/input/blood-vessel-segmentation/\"\n","parhs=[\"/kaggle/input/blood-vessel-segmentation/train/kidney_1_dense\"]\n","for i,path in enumerate(parhs):\n","    if path==\"/kaggle/input/blood-vessel-segmentation/train/kidney_3_dense\":\n","        continue\n","    x=load_data(glob(f\"{path}/images/*\"),is_label=False)\n","    print(x.shape)\n","    y=load_data(glob(f\"{path}/labels/*\"),is_label=True)\n","    print(y.shape)\n","    train_x.append(x)\n","    train_y.append(y)\n","\n","    #(C,H,W)\n","\n","    #aug\n","    train_x.append(x.permute(1,2,0))\n","    train_y.append(y.permute(1,2,0))\n","    train_x.append(x.permute(2,0,1))\n","    train_y.append(y.permute(2,0,1))\n","path1=\"/kaggle/input/blood-vessel-segmentation/train/kidney_3_sparse\"\n","path2=\"/kaggle/input/blood-vessel-segmentation/train/kidney_3_dense\"\n","paths_y=glob(f\"{path2}/labels/*\")\n","paths_x=[x.replace(\"labels\",\"images\").replace(\"dense\",\"sparse\") for x in paths_y]\n","\n","val_x=load_data(paths_x,is_label=False)\n","print(val_x.shape)\n","val_y=load_data(paths_y,is_label=True)\n","print(val_y.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T20:27:45.28857Z","iopub.status.busy":"2024-01-11T20:27:45.288132Z"},"trusted":true},"outputs":[],"source":["tc.backends.cudnn.enabled = True\n","tc.backends.cudnn.benchmark = True\n","    \n","train_dataset=Kaggld_Dataset(train_x,train_y,arg=True)\n","train_dataset = DataLoader(train_dataset, batch_size=CFG.train_batch_size ,num_workers=2, shuffle=True, pin_memory=True)\n","val_dataset=Kaggld_Dataset([val_x],[val_y])\n","val_dataset = DataLoader(val_dataset, batch_size=CFG.valid_batch_size, num_workers=2, shuffle=False, pin_memory=True)\n","\n","model=build_model()\n","model=DataParallel(model)\n","\n","loss_fc=DiceLoss()\n","#loss_fn=nn.BCEWithLogitsLoss()\n","optimizer=tc.optim.AdamW(model.parameters(),lr=CFG.lr)\n","scaler=tc.cuda.amp.GradScaler()\n","scheduler = tc.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.lr,\n","                                                steps_per_epoch=len(train_dataset), epochs=CFG.epochs+1,\n","                                                pct_start=0.1,)\n","for epoch in range(CFG.epochs):\n","    model.train()\n","    time=tqdm(range(len(train_dataset)))\n","    losss=0\n","    scores=0\n","    for i,(x,y) in enumerate(train_dataset):\n","        x=x.cuda().to(tc.float32)\n","        y=y.cuda().to(tc.float32)\n","        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n","        x=add_noise(x,max_randn_rate=0.5,x_already_normed=True)\n","        \n","        with autocast():\n","            pred=model(x)\n","            loss=loss_fc(pred,y)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad()\n","        scheduler.step()\n","        score=dice_coef(pred.detach(),y)\n","        losss=(losss*i+loss.item())/(i+1)\n","        scores=(scores*i+score)/(i+1)\n","        time.set_description(f\"epoch:{epoch},loss:{losss:.4f},score:{scores:.4f},lr{optimizer.param_groups[0]['lr']:.4e}\")\n","        time.update()\n","        del loss,pred\n","    time.close()\n","    \n","    model.eval()\n","    time=tqdm(range(len(val_dataset)))\n","    val_losss=0\n","    val_scores=0\n","    for i,(x,y) in enumerate(val_dataset):\n","        x=x.cuda().to(tc.float32)\n","        y=y.cuda().to(tc.float32)\n","        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n","\n","        with autocast():\n","            with tc.no_grad():\n","                pred=model(x)\n","                loss=loss_fc(pred,y)\n","        score=dice_coef(pred.detach(),y)\n","        val_losss=(val_losss*i+loss.item())/(i+1)\n","        val_scores=(val_scores*i+score)/(i+1)\n","        time.set_description(f\"val-->loss:{val_losss:.4f},score:{val_scores:.4f}\")\n","        time.update()\n","\n","    time.close()\n","tc.save(model.module.state_dict(),f\"./{CFG.backbone}_{epoch}_loss{losss:.2f}_score{scores:.2f}_val_loss{val_losss:.2f}_val_score{val_scores:.2f}_midd_1024.pt\")\n","\n","time.close()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6962461,"sourceId":61446,"sourceType":"competition"},{"datasetId":1074109,"sourceId":1807973,"sourceType":"datasetVersion"},{"sourceId":150248402,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
